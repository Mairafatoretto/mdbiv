%  template.tex for Biometrics papers
%
%  This file provides a template for Biometrics authors.  Use this
%  template as the starting point for creating your manuscript document.
%  See the file biomsample.tex for an example of a full-blown manuscript.



\documentclass[useAMS,referee]{biom}

\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{blkarray}
\usepackage{arydshln}
\usepackage{multirow}
\usepackage{mathtools}
\usepackage{amsmath,bm}
\usepackage{listings} 
\usepackage{blkarray}
\usepackage{footnote}
\usepackage{arydshln}
\usepackage{mathtools}
\usepackage{endrotfloat}
\usepackage{footnote}
\usepackage{natbib}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[figurename=Figure]{caption}
\usepackage[tablename=Table]{caption}
\usetikzlibrary{matrix}
\usepackage{arydshln}
\usepackage{caption}


%  If your system does not have the AMS fonts version 2.0 installed, then
%  remove the useAMS option.
%
%  useAMS allows you to obtain upright Greek characters.
%  e.g. \umu, \upi etc.  See the section on "Upright Greek characters" in
%  this guide for further information.
%
%  If you are using AMS 2.0 fonts, bold math letters/symbols are available
%  at a larger range of sizes for NFSS release 1 and 2 (using \boldmath or
%  preferably \bmath).
% 
%  Other options are described in the user guide. Here are a few:
% 
%  -  If you use Patrick Daly's natbib  to cross-reference your 
%     bibliography entries, use the usenatbib option
%
%  -  If you use \includegraphics (graphicx package) for importing graphics
%     into your figures, use the usegraphicx option
% 
%  If you wish to typeset the paper in Times font (if you do not have the
%  PostScript Type 1 Computer Modern fonts you will need to do this to get
%  smoother fonts in a PDF file) then uncomment the next line
%  \usepackage{Times}

%%%%% PLACE YOUR OWN MACROS HERE %%%%%

\def\bSig\mathbf{\Sigma}
\newcommand{\VS}{V\&S}
\newcommand{\tr}{\mbox{tr}}

%  The rotating package allows you to have tables displayed in landscape
%  mode.  The rotating package is NOT included in this distribution, but
%  can be obtained from the CTAN archive.  USE OF LANDSCAPE TABLES IS
%  STRONGLY DISCOURAGED -- create landscape tables only as a last resort if
%  you see no other way to display the information.  If you do do this,
%  then you need the following command.

%\usepackage[figuresright]{rotating}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%  Here, place your title and author information.  Note that in 
%  use of the \author command, you create your own footnotes.  Follow
%  the examples below in creating your author and affiliation information.
%  Also consult a recent issue of the journal for examples of formatting.

\title[A bivariate framework to jointly model count and continuous responses]{A bivariate framework to jointly model count and continuous responses}

%  Here are examples of different configurations of author/affiliation
%  displays.  According to the Biometrics style, in some instances,
%  the convention is to have superscript *, **, etc footnotes to indicate 
%  which of multiple email addresses belong to which author.  In this case,
%  use the \email{ } command to produce the emails in the display.

%  In other cases, such as a single author or two authors from 
%  different institutions, there should be no footnoting.  Here, use
%  the \emailx{ } command instead. 

%  The examples below corrspond to almost every possible configuration
%  of authors and may be used as a guide.  For other configurations, consult
%  a recent issue of the the journal.

%  Single author -- USE \emailx{ } here so that no asterisk footnoting
%  for the email address will be produced.

%\author{John Author\emailx{email@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Two authors from the same institution, with both emails -- use
%  \email{ } here to produce the asterisk footnoting for each email address

%\author{John Author$^{*}$\email{author@address.edu} and
%Kathy Authoress$^{**}$\email{email2@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K.}

%  Exactly two authors from different institutions, with both emails  
%  USE \emailx{ } here so that no asterisk footnoting for the email address
%  is produced.

\author
{Maíra Blumer Fatoretto\emailx{mairafatoretto@gmail.com} \\
Department of Exact Sciences, University of São Paulo, Piracicaba, São Paulo, Brazil.
\and
Caroline Brophy\emailx{caroline.brophy@tcd.ie} \\
School of Computer Science and Statistics, Trinity College Dublin, Dublin 2, Ireland.
\and
Clarice Garcia Borges Demétrio\emailx{clarice.demetrio@usp.br} \\
Department of Exact Sciences, University of São Paulo, Piracicaba, São Paulo, Brazil.
\and
Rafael de Andrade Moral\emailx{Rafael.DeAndradeMoral@mu.ie} \\
Departament of Mathematics and Statistics, Maynooth University, Maynooth, Ireland.}

%  Three or more authors from same institution with all emails displayed
%  and footnoted using asterisks -- use \email{ } 

%\author{John Author$^*$\email{author@address.edu}, 
%Jane Author$^{**}$\email{jane@address.edu}, and 
%Dick Author$^{***}$\email{dick@address.edu} \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K}

%  Three or more authors from same institution with one corresponding email
%  displayed

%\author{John Author$^*$\email{author@address.edu}, 
%Jane Author, and Dick Author \\
%Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K}

%  Three or more authors, with at least two different institutions,
%  more than one email displayed 

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Kathy Author$^{2,**}$\email{anotherauthor@address.edu}, and 
%Wilma Flinstone$^{3,***}$\email{wilma@bedrock.edu} \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Biostatistics, University of North Carolina at 
%Chapel Hill, Chapel Hill, North Carolina, U.S.A. \\
%$^{3}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}

%  Three or more authors with at least two different institutions and only
%  one email displayed

%\author{John Author$^{1,*}$\email{author@address.edu}, 
%Wilma Flinstone$^{2}$, and Barney Rubble$^{2}$ \\
%$^{1}$Department of Statistics, University of Warwick, Coventry CV4 7AL, U.K \\
%$^{2}$Department of Geology, University of Bedrock, Bedrock, Kansas, U.S.A.}


\begin{document}

%  This will produce the submission and review information that appears
%  right after the reference section.  Of course, it will be unknown when
%  you submit your paper, so you can either leave this out or put in 
%  sample dates (these will have no effect on the fate of your paper in the
%  review process!)

%\date{{\it Received October} 2007. {\it Revised February} 2008.  {\it
%Accepted March} 2008.}

%  These options will count the number of pages and provide volume
%  and date information in the upper left hand corner of the top of the 
%  first page as in published papers.  The \pagerange command will only
%  work if you place the command \label{firstpage} near the beginning
%  of the document and \label{lastpage} at the end of the document, as we
%  have done in this template.

%  Again, putting a volume number and date is for your own amusement and
%  has no bearing on what actually happens to your paper!  



%  This label and the label ``lastpage'' are used by the \pagerange
%  command above to give the page range for the article.  You may have 
%  to process the document twice to get this to match up with what you 
%  expect.  When using the referee option, this will not count the pages
%  with tables and figures.  

\label{firstpage}

%  put the summary for your paper here

\begin{abstract}
Multivariate data are present in many studies in the natural sciences, including entomology (study of insects). Generally, the interest is in more than one response variable and in how factors and covariates affect them, simultaneously. However, when the interest is in one count variable and one continuous variable, we need to use different approaches to model these variables jointly. In this work, motivated by a data set from ​​entomology, in which a continuous and a count response were observed simultaneously, a mean-dispersion bivariate model was developed. The proposed model is based on the bivariate normal  distribution, and adds a new parameter to the variance-covariance matrix to better accommodate the main characteristics of count data, such as overdispersion or underdispersion. This modelling framework showed good results and also allowed for greater flexibility in the case of heterogeneity of variances, and allows for the correlation between the two responses to vary according to an experimental treatment.
\end{abstract}

%  Please place your key words in alphabetical order, separated
%  by semicolons, with the first letter of the first word capitalized,
%  and a period at the end of the list.
%

\begin{keywords}
Count data; Entomology; Joint modelling; Multivariate data; Poisson model.
\end{keywords}

%  As usual, the \maketitle command creates the title and author/affiliations
%  display 

\maketitle

%  If you are using the referee option, a new page, numbered page 1, will
%  start after the summary and keywords.  The page numbers thus count the
%  number of pages of your manuscript in the preferred submission style.
%  Remember, ``Normally, regular papers exceeding 25 pages and Reader Reaction 
%  papers exceeding 12 pages in (the preferred style) will be returned to 
%  the authors without review. The page limit includes acknowledgements, 
%  references, and appendices, but not tables and figures. The page count does 
%  not include the title page and abstract. A maximum of six (6) tables or 
%  figures combined is often required.''

%  You may now place the substance of your manuscript here.  Please use
%  the \section, \subsection, etc commands as described in the user guide.

%

\section{Introduction}
\label{s:intro}

In many observational or experimental studies, multiple response variables are of interest, and they may be correlated. In statistical modelling techniques, multiple response variables can be analyzed in isolation using univariate methods, where the dependences between them are not considered in the modelling. However, the correlation between the response variables can provide valuable insight, and their incorporation in the modelling framework can lead to more reliable tests and improved inference, especially when this correction is of high magnitude. Multivariate regression techniques facilitate the inclusion of correlated responses and are useful for exploring data patterns that may exist in more than one dimension ~\citep{raykov2008introduction,everitt2011introduction}. 

For the majority of univariate analysis, inferences are based on the normal distribution, and many multivariate techniques are extensions of univariate analysis, i.e., the majority of multivariate procedures have the multivariate normal distribution as their underpinning. For example, in bivariate analysis, the relationship between two continous variables with unbounded symmetric distribution could be described by a joint probability distribution using the bivariate normal density~\citep{jobson2012applied}. However, other distributions according to characteristics of the response variables can also be considered. The analysis of non-normal univariate or multivariate data involves, mostly, the generalized linear models (GLM) formulated by Nelder and Wedderburn (1972),~\nocite{nelder1972generalized} that represent an elegant and encompassing mathematical framework to model response variables whose distribution belongs to the exponential family (such as normal, binomial, Poisson, gamma and inverse Gaussian). A feature of the exponential family of distributions is the  \textit{mean-variance} relationship, i.e., the fact that the variance is a function of the mean. 

Johnson (1997) presented a range of discrete multivariate models that can jointly analyze count and binary data. Nevertheless, when it comes to joint modelling of one discrete and one continuous variable, suitable techniques and current software implementations are still emerging ~\citep{bonat2016multivariate}. Bonat et. al. (2018) proposed a flexible framework to model response variables of different nature jointly (e.g., unbounded and bounded continuous variables and discrete variables). They created the multivariate covariance generalized linear modelling framework (McGLM), implemented as the \texttt{mcglm} package for \texttt{R software} (R CORE TEAM, 2020).~\nocite{team2020r} In their approach, marginal models are fitted through quasi-likelihood functions based on first and second-moment assumptions~\citep{bonat2017modelling,bonat2018multiple}. The framework includes flexibility in the specification of the covariance strucure and provides reliable tests for fixed effects. However, one drawback of this framework is that it does not provide an associated probability distribution. Another possibility for modelling discrete and continuous multivariate response variables jointly are copula models~\citep{nikoloulopoulos2009modeling,krupskii2013factor}. The copula model allows us to incorporate the correlation between the variables and also allows flexibility when choosing the correlation structure. However, the use of different copulas generates completely different results. In this paper, we present another option for modelling  count and continuous data jointly using an approximation based on the central limit theorem and a one-parameter extension of the variance-covariance structure.

When analyzing count data, the Poisson model is a natural first choice, however, the Poisson distribution can be approximated asymptotically by the normal distribution~\citep{mood1950introduction}. A multivariate extension based on the normal distribution is advantageous is the sense that this distribution presents known and closed formulas.  A drawback for the Poisson model is the assumption of equality of mean and variance, which is not met very often in practice. This extra overdispersion in count data may be caused by a deficiency of relevant covariates or heterogeneity of samples, or repeated measures, and it is necessary to model this extra-variability to obtain reliable inference about the parameters ~\citep{hinde1998overdispersion,ver2007quasi}. These causes of variation can also impact continuous data. However, the normal distribution, often used to fit to this data,  assumes homoscedastic variance, which could not reflect the real characteristics of the data.  Aitkin (1987) proposed the joint modelling of mean and dispersion in the normal regression analysis using a Fisher scoring algorithm for the simultaneous maximum likelihood estimation. This proposal brought a powerful approach to deal with the heterogeneity of variance, which was modeled rather than being transformed away. McCullagh and Nelder (1989)~\nocite{mccullagh1989generalized} presented the joint modelling of mean and dispersion in the generalized linear models, using the extended quasi-likelihood function~\citep{nelder1987extended}. 





In this paper, we present a joint modelling of bivariate data, which also could be extended to multivariate data of more than two responses, to accommodate two counts, or two continuous response variables, or a combination of one count and one continuous variable using the bivariate normal approximation.  To obtain a versatile framework to deal with count data, we included an extra parameter in the variance-covariance matrix to capture different characteristics present in this type of data, making it very useful also for continuous data.  The normal bivariate model proposed allows us to model jointly mean and dispersion dealing with over- and underdispersion, and heteroscedastic variance. Furthermore, it is possible to fit the model allowing the correlation structure to depend on experimental conditions, for example, the correlation between two responses may change according to the levels of an experimental treatment. To exemplify the method developed, a case study applied to the area of entomology is presented.


This paper is organized as follows. Section 2 presents the case-study that is a motivation for this paper. Section 3 presents the bivariate framework to jointly model count and continuous responses.  Section 4 presents estimation and inference for the novel regression model based on the likelihood paradigm. The case study analysis and results are presented in Section 5. The properties of the maximum likelihood and profile likelihood estimators are assessed in Section 6 through simulation
studies. Finally, we present discussion in Section 7.

% Assuming that there is this normal approximation for the count variables, we can incorporate new parameters in the variance-covariance matrix, modeling subdispersion and overdispersion even for multivariate data. 

% Therefore, the most fundamentals assumptions in multivariate analysis are normality,  homoscedasticity, independence of error, and linearity. Each of these issues should be addressed to some extent for each application of a multivariate technique ~\citep{hair1998multivariate}.


%Several models were proposed to model these errors, such as, the Mixed Models~\citep{carroll1988transformation,pinheiro2006mixed} and, the Generalized Linear Mixed Models for the GLM approach ~\citep{breslow1993approximate}. 




%introduced by Pregibon (1984)~\nocite{pregibon1984p}. In this approach the joint model is specified in terms of the dependence on covariates of the first two moments and the interlinking suggests an algorithm for the fit, once, the mean depends on the dispersion and the dispersion requires an estimate of the mean and, the estimated is assess






%In this paper, we present a new model for the analysis of bivariate data consisting of one count and one continuous variable, or both count or continuous variable, using a bivariate normal approximation. However this method coud also be extend to multivariate data. To obtain a versatility framework to deal with count data, we include a new parameters in the variance-covariance to capture different characteristics
%present in these distributions. 

%The double normal bivariate model proposed here is very flexible. First, it is possible to model jointly two responses, as in the classical bivariate model, and to considerer the correlation between them to estimate the others parameters present, even when there are different correlation, for example, by covariates. Thereby, the inferences could identify characteristics that were not possible in the univariate framework, for example, differences between treatments when the standard error taking account the correlation between the response variables. Also, this model include joint modelling of mean and dispersion, dealing with equal dispersion, overdispersion, underdispersion, and heteroscedastic errors, characteristics common in many structures of data, as count variables. 


\section{Case study}


The data comes from an experiment examining \textit{Podisus nigrispinus} (Hemiptera: Pentatomidae) a predatory stinkbug, found in agricultural and forest systems in several countries of Central and South America, when fed two different prey \textit{(Anticarsia gemmatalis} and \textit{Diatraea saccharalis}, two agricultural pests). 
This stinkbug has an important role as a control agent
biological for different cultures, for example, in the forest area, the predator can be useful in the biological control of defoliating caterpillars in \textit{Eucalyptus} plantations. 

Given this importance,  experiments aimed at assessing the dynamics of prey consumption by \textit{P. nigrispinus} have been developed. In these studies, the response variables are the weight and the fertility of the female predator. The weight is one way to measure the development of an insect over its life cycle, and the fertility measures how well the insect can establish in an ecosystem. This information is essential for the choice of biological control strategies in pest management programs~\citep{parra2002controle}. 

The experiment was carried out at the Laboratory of Forest Ecology and Entomology, Department of Entomology and Acarology, ESALQ - USP, Piracicaba, Brazil. The insects were placed in individualized incubator chambers, where they were fed and monitored daily.  The two diets or treatments consisted of offering three caterpillars of either  \textit{A. gemmatalis} or \textit{D. saccharalis} daily. The experiment started with 50 female stinkbug insects for each diet, however, as there was a high mortality rate, the final measurements were made only with 9 stinkbug replicates of the  \textit{A. gemmatalis} treatment and 18 of the \textit{D. saccharalis} treatment.  After 18 days of receiving the diet, the weight of females was measured. Next, the females were allocated with a male that also received the same diet. The formed couples were kept until the death of the females. All eggs laid by the females were placed in Petri dishes and counted. The correlation between the two outcomes (female weight and number of eggs) was examined by a scatterplot (Figure \ref{fig1.1}).
This indicates that stinkbugs being fed the \textit{A. gemmatalis} caterpillar diet performed more poorly overall than those fed the \textit{D. saccharalis} caterpillar diet. A strong correlation between female weight and number of eggs was suggested for the \textit{D. saccharalis} diet; this indicates that a modelling approach that allows the strength of the correlation between bivariate responses to change depending on an experimental treatment would be suitable here. 

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.8\textwidth]{pod1}
	\caption{Scatterplot of \textit{Podisus nigrispinus} female weight after 18 days and total number of eggs laid throughout the insect's life cycle
		by treatment (a diet consisting of \textit{Anticarsia gemmatalis} or \textit{Diatraea saccharalis}).}
	\label{fig1.1}
\end{figure}


\section{Joint Modelling of a Count and a Continuous Response
}
\label{s:model}

Bivariate models may be applied to analyze bivariate data jointly. However, when the outcomes have different distributions, the useful bivariate normal may not capture all information present in these data. In this paper, we propose a joint modelling approach, based on the multivariate normal distribution, that is suitable for jointly analyzing count and continuous bivariate data. 

The marginal distribution structure is the same for either a count or a continuous variable. Both assume a normal distribution, with mean $\boldsymbol{\mu_i}; i=1,2$ and standard deviation depending on the mean and a dispersion parameter $(\boldsymbol{\phi_i}\boldsymbol{\mu_i}; i=1,2)$. It is also possible to capture the heterogeneity of variances, overdispersion, or underdispersion through the modelling of the $\boldsymbol{\phi}$ parameters. We model the mean $\boldsymbol{\mu}$ through a monotonic and differentiable link function $\boldsymbol{g(\mu) = \eta}$, such that the linear predictor is $\boldsymbol{\eta} = \mathbf{X}\boldsymbol{\beta}$.

Let $\mathbf{Y_1}$ and $\mathbf{Y_2}$ be vectors of count or continuous random variables. We then assume that marginally $\mathbf{Y_1} \sim N(\boldsymbol{\mu_1},\boldsymbol{\Sigma_1})$ and  $\mathbf{Y_2} \sim N(\boldsymbol{\mu_2},\boldsymbol{\Sigma_2})$, with the joint distribution of $\mathbf{Y_1}$ and $\mathbf{Y_2}$ taken to be multivariate normal, i.e.,

\begin{align}\label{eq1}
\begin{bmatrix}\mathbf{Y_1}\\
\mathbf{Y_2}
\end{bmatrix} &\sim  N
\begin{pmatrix}
\begin{bmatrix}
\boldsymbol{\mu_1}\\
\boldsymbol{\mu_2}
\end{bmatrix}\!\!,&
\begin{bmatrix}
\boldsymbol{\Sigma_1} & \boldsymbol{\Sigma_{12}} \\
\boldsymbol{\Sigma_{12}} & \boldsymbol{\Sigma_2}
\end{bmatrix}
\end{pmatrix},
\end{align}
where  $\boldsymbol{\Sigma_1}$ denotes the covariance matrix for the first response; $\boldsymbol{\Sigma_2}$ denotes the covariance matrix for the second response and $\boldsymbol{\Sigma_{12}}$
denotes the covariance matrix between the two responses, where $\boldsymbol{\Sigma_{12}}$ depends on a correlation parameter $\rho$, $\boldsymbol{\Sigma_1}$ and $\boldsymbol{\Sigma_2}$.

We write  $\boldsymbol{\Sigma_i} = \mbox{diag}(\mathbf{X}_\phi \boldsymbol{\phi}) \mbox{diag} (g^{-1}(\mathbf{X}_\beta \boldsymbol{\beta}))  = \mathbf{D_\phi}\mathbf{D_\mu}$, where the function $\mbox{diag}(\mathbf{x})$ is defined as a square matrix with the elements of the vector $\mathbf{x}$ in its main diagonal; $\mathbf{X_\phi}$ and $\mathbf{X_\beta}$ are the design  matrices for dispersion and regression parameters respectively; and $\boldsymbol\phi$ and $\boldsymbol\beta$ unknown parameter vectors. We may write for $\mu_{ij}$ and $\phi_{ij}$
\begin{center}
	$\boldsymbol{\Sigma_i} =
	\begin{pmatrix}
	\phi_{i1}\mu_{i1} & 0 & \dots & 0 \\
	0 & \phi_{i2}\mu_{i2} & \dots & 0 \\
	\vdots & \vdots & \ddots & \vdots \\
	0 & 0 & \dots & \phi_{in}\mu_{in}
	\end{pmatrix}
	$,
\end{center}
where $i$ represents the response number (i = 1,2); $j$ is the observation ($j = 1, \dots, n$) and the variance-covariance matrix ($\boldsymbol{\Sigma_i}$) will involve the mean and dispersion parameters.


\section{Estimation and Inference}
\subsection{Univariate modelling}\label{univariate}

Taking one of the response outcomes $\mathbf {Y} _i $, from here we will omit the index $ i $ to improve readability. We assume that asymptotically  $\boldsymbol{Y} \sim N({\boldsymbol{\mu}},\boldsymbol{\Sigma}= \mathbf{D_\phi}\mathbf{D_\mu})$, and write the log-likelihood as
\begin{equation}\label{eq22}
\begin{array}{l}
\log \mathcal{L}(\boldsymbol{\mu},\boldsymbol{\Sigma}| \mathbf{y})= l(\boldsymbol{\mu},\boldsymbol{\Sigma}| \mathbf{y})= -\frac{1}{2}\{\ln |\boldsymbol{\Sigma}| +  (\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1} (\mathbf{y}-\boldsymbol{\mu})\} + \mbox{const.}



%= -\frac{1}{2}\log |\mathbf{D_\phi}\mathbf{D_\mu}| - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu})\\~\\

%  =	\{- \frac{1}{2}\log|\mathbf{D_\phi}|-  \frac{1}{2} \log|\mathbf{D_\mu}|  - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu}) \}

\end{array}
\end{equation}

The estimating equations can be obtained using the chain rule. Differentiating (\ref{eq22}) w.r.t. $\boldsymbol{\Sigma}$ yields 

\begin{equation}\label{eq4}
\begin{array}{l}
\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} = -\frac{1}{2}(\boldsymbol{\Sigma}^{-1})^T+\frac{1}{2}(\boldsymbol{\Sigma}^{-1})^T(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'(\boldsymbol{\Sigma}^{-1})^T.

\end{array}
\end{equation}

Now further diffferentiating eq. (\ref{eq4}) w.r.t. $\mathbf{D_\phi}$ and  w.r.t. $\boldsymbol{\phi}$ yields (see Appendix A for calculations):
\begin{equation}\label{eq5}
\begin{array}{l}
-\frac{1}{2}\mbox{diag}\mathbf{ (X_\phi \boldsymbol{\phi})^{-1}}\circ \mathbb{I}+\frac{1}{2}\mbox{diag}\mathbf{ (X_\phi \boldsymbol{\phi})^{-1}D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mbox{diag}\mathbf{( X_\phi \boldsymbol{\phi})^{-1}}\circ \mathbb{I}.
\end{array}
\end{equation}


Finally, solving eq. (\ref{eq5}) w.r.t. $\boldsymbol{\phi}$ yields: 
\begin{equation}\label{eq6}
\boldsymbol{\hat{\phi}} = \mathbf{(X_\phi'X_\phi)^{-1}X_\phi'}\mathbf{\{(D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\circ \mathbb{I}) \mathbb{J}\},
\end{equation}
where $\mathbb{I}$ is an $n \times n$ identity matrix and $\mathbb{J}$ is an $n \times 1$ vector of 1s, and $\circ$ the Hadamard product. In this case, for positive values, the dispersion parameter ($\phi$) is always greater than zero. If $\phi < 1$ there is underpersion and $\phi>1$ overdispersion.

It is not as straightforward to obtain the maximum likelihood estimator for $\boldsymbol{\beta}$, because $\boldsymbol{\mu}$ is also a function of $\boldsymbol{\Sigma}$ in the likelihood. Therefore we firstly approximate the estimate of the regression coefficients using the known estimator for the regression parameters in the linear models and then update $\hat{\boldsymbol{\beta}}$ using $\boldsymbol{\hat{\mu}}$ estimated in the preceding stage.  The maximum likelihood estimator (MLE) of $\boldsymbol{\beta}$, conditional on $\boldsymbol{\mu},\boldsymbol{\phi},\boldsymbol{\rho}$, with $\boldsymbol{\Sigma(\mu},\boldsymbol{\phi},\boldsymbol{\rho})$ is given by ~\citep{laird1982random,molenberghs2000linear}
\begin{center}
$\boldsymbol{\hat{\beta}(\mu},\boldsymbol{\phi},\boldsymbol{\rho}) =  \mathbf{(X^{T}\boldsymbol{\Sigma}}^{-1}\mathbf{X)^{-1} X^{T}\boldsymbol{\Sigma}}^{-1}\mathbf{Y}$.
\end{center} 


Because we are including different link functions,  not limited to identity, we included a matrix of weights $\mathbf{W}$ in the estimation of $\boldsymbol{\beta}$, following the estimation procedure of the GLM framework. The estimation for the vector $\boldsymbol{\beta}$ is given considering
\begin{equation}
\boldsymbol{\eta} = g(\boldsymbol{\mu})\textnormal{,}  \qquad \mathbf{W} = \dfrac{\textnormal{1}}{\mathbf{V}(\boldsymbol{\mu})[g'(\boldsymbol{\mu})]^2} \quad \mbox{and} \quad \boldmath{\textbf{z} = \eta + \textbf{G}(\textbf{y} - \mu)}\textnormal{,}
\end{equation}  
where $\textbf{W}$ is a diagonal matrix of weights, $\mathbf{V}(\boldsymbol{\mu}) = \mathbf{D_\phi D_\mu}$ and $\textbf{G} =$diag$\{g'(\mu_1),\dots,g'(\mu_n)\}$ ~\citep{nelder1972generalized,mccullagh1989generalized}.

Using $\boldsymbol{\eta^{(1)}} = g(\mathbf{y})$ for the first iteration, then



\begin{equation}\label{estimatorregression}
\hat{\boldsymbol{\beta}}^{(1)}= \mathbf{(X^{T}X)^{-1} X^{T}\boldsymbol{\eta}}^{(1)}\qquad \mbox{and} \qquad \hat{\boldsymbol{\beta}}^{(n)} = \mathbf{(X^{T}W}^{(n-1)}\mathbf{X)^{-1} X^{T}W}^{(n-1)}\textbf{z}^{(n-1)}.
\end{equation}

This algorithm yields the same results as numerically maximising the likelihood function using, e.g. the BFGS algorithm, however it is much faster in terms of computational burden. The behavior of these estimates is tested in the simulation studies in Section 6.

%Para $m=2, \dots , k$, sendo $k-1$ o número necessário de iterações para convergência, pode ser resumido as estimativas:\\
%(1) Obter as estimativas\\

%\begin{center}
%	$\boldsymbol{\eta}^{m} = \mathbf{X}\boldsymbol{\beta}^{(m)}$ and $\boldsymbol{\mu}^{(m)} = \exp \{\mathbf{X}\boldsymbol{\beta}^{(m)}\}$
%\end{center}

%(2) Obter a variável dependente ajustada:
%\begin{center}
%$\mathbf{z^{(m)}} = \log(\boldsymbol{\mu}^{(m)})+ \dfrac{(\mathbf{y} - \boldsymbol{\mu}^{(m)})}{\boldsymbol{\mu}^{(m)}}$
%\end{center}
%e os pesos

%\begin{center}
%	$\mathbf{w^{(m)}} = \dfrac{\boldsymbol{\phi}}{\boldsymbol{\mu^{(m)}}}$
%\end{center}
%and 
%(3) calcular

%\begin{center}
%	$	\boldsymbol{\beta}^{(m+1)} = (\mathbf{X^{T}W^{(m)}X)^{-1} X^ {T}W^{(m)}z^{(m)}}$
%\end{center}


\subsection{Bivariate modelling}

To model both outcomes jointly it is necessary to estimate the matrix $\boldsymbol{\Sigma}_{12}$ (eq. \ref{eq1}). Martinez and Benedito (2013)~\nocite{martinez2013general}introduced a method to construct the multivariate covariance matrix using Kronecker products. Bonat and J{\o}rgersen (2016) applied the same method to develop the technique called Multivariate Covariance Generalized Linear Models.
Let $\mathbf{Y}_{nR \times 1} = \{\mathbf{Y}_1, \dots ,\mathbf{Y}_R\} $, where the response variables are stacked, with $n = \mbox{number of observations}$ and $R = \mbox{number of outcomes}$; $\boldsymbol{\Sigma}_i$ the $n \times n$ covariance matrix within the outcome $i$ for $i = 1,\dots, R$. Let $\boldsymbol{\Sigma}_b$ be the $R \times R$ correlation matrix between outcomes, with the diagonal elements equal to $1$ and off-diagonal elements equal to $\rho$. 
Then 
\begin{equation*}
\mbox{E}(\mathbf{Y})	 = \{g^{-1}(\mathbf{X}_1 \boldsymbol{\beta}_1), \dots ,g^{-1}(\mathbf{X}_R \boldsymbol{\beta}_R)\} 
\end{equation*}
\begin{equation}\label{variance}	
\mbox{Var}(\mathbf{Y})=\boldsymbol{\Sigma}= \mbox{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1, \dots , \tilde{\boldsymbol{\Sigma}}_R)(\boldsymbol{\Sigma}_b \otimes \mathbf{I} )\mbox{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1^{T}, \dots , \boldsymbol{\tilde{\Sigma}}_R^{T})
\end{equation}
where the matrix $\boldsymbol{\tilde{\Sigma}}_i$ denotes the lower triangular matrix of the Cholesky decomposition of $\boldsymbol{\Sigma}_i$. The operator Bdiag denotes a block diagonal matrix and $\mathbf{I}$ denotes an $n \times n$ identity matrix. 

%Then, the covariance matrix, could be defined as
%\begin{center}
%$\boldsymbol{\Sigma}_{12} = \rho  \tilde{\boldsymbol{\Sigma}}_i \tilde{\boldsymbol{\Sigma}}_{i'}^{T} $
%\end{center}
%with $i \neq i'$.

Here we propose an extension to allow more than one correlation coefficient, for example, we could have one for each treatment in our case study data (Section 2). This requires increasing the flexibility of the $(\boldsymbol{\Sigma}_b\otimes \mathbf{I})$ matrix to include more than one correlation coefficient.   
Let $\mathbf{X}_\rho$ denote a $N \times t$ design matrix, with $t$ the to number of treatment levels, and $\boldsymbol{\rho}$ a vector for the correlation coefficients  with $t \times 1$ dimension, then w.r.t
\begin{center}
$\boldsymbol{\Sigma}_\rho=	\mbox{diag}(\mathbf{X}_\rho \boldsymbol{\rho})$, 
\end{center}
and replacing $(\boldsymbol{\Sigma}_b\otimes \mathbf{I})$ in the eq. (\ref{variance}), we have

\begin{equation*}	
\mbox{Var}(\mathbf{Y})=\boldsymbol{\Sigma}= \mbox{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1, \tilde{\boldsymbol{\Sigma}}_2)
\left[\begin{array}{cc}
\mathbf{I} & \boldsymbol{\Sigma}_\rho\\
\boldsymbol{\Sigma}_\rho & \mathbf{I} \end{array} \right]
\mbox{Bdiag}(\tilde{\boldsymbol{\Sigma}}_1^{T}, \boldsymbol{\tilde{\Sigma}}_2^{T}).
\end{equation*}

Hence, the proposed model is considering $R=2$ outcomes. Then, for $\mathbf{Y} = \mathbf{[Y_1,Y_2]}$, the full log likelihood is
\begin{equation}\label{16}
l(\boldsymbol{\mu},\boldsymbol{\Sigma(\mu,\phi,\rho)}| \mathbf{y}) = -\frac{1}{2}\{\ln |\boldsymbol{\Sigma}| +  (\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1} (\mathbf{y}-\boldsymbol{\mu})\} + \mbox{const}.
\end{equation}



Here the covariance matrix is more flexible, allowing the modelling of data sets with different correlation profiles. A likelihood ratio test can be used to test if correlations coefficients differ, for example by levels of a treatment in an experiment. 

J{\o}rgensen and Knudsen (2004)~\nocite{jorgensen2004parameter}suggested a method to jointly estimate all parameters of the $\boldsymbol{\Sigma}$ matrix using the Newton scoring algorithm, based on quasi-likelihood and the Pearson estimating functions, applying second-moment assumptions. As an alternative approach, here, the matrices $\boldsymbol{\Sigma_1}$ and $\boldsymbol{\Sigma_2}$ are estimated first separately, using the maximum likelihood method, presented in section (\ref{univariate}). These estimates are then used as initial values to obtain the estimates $\boldsymbol{\hat\rho}$ by profiling the log-likelihood function w.r.t. $\boldsymbol{\rho}$ and maximizing (\ref{16}) numerically. At each step, the estimates for $\boldsymbol{\mu}$ and $\boldsymbol{\phi}$ are updated, using the estimation algorithm described in Section 4.1. Since the derivatives of $l$ w.r.t. $\boldsymbol{\rho}$ cannot be obtained in closed form, the  \texttt{L-BFGS-B} algorithm~\citep{zhu1997algorithm} was implemented in the \texttt{mle2()} function for \texttt{R} software (R CORE TEAM, 2020).~\nocite{team2020r} Standard errors for the regression parameters are obtained based on the observed information matrix $\mathbf{I}(\boldsymbol\theta)$, where $\textbf{I}\boldsymbol(\theta)= - \textbf{H}\boldsymbol(\theta)$ (hessian matrix) is \textcolor{red}{calculated by numerical approximation of $l(\boldsymbol\theta)$}, using the \texttt{hessian()} function of the package  \texttt{numDeriv} ~\citep{gilbert2006numderiv}. Since $\hat{\boldsymbol{\phi}}$ depends on $\hat{\boldsymbol{\beta}}$, the standard errors for the dispersion parameters were obtained using a two-stage algorithm. \textcolor{red}{First, considering $\boldsymbol{\hat\rho}$, we obtained the  standard errors for $\boldsymbol{\hat\beta}$ . In the next stage considering $\boldsymbol{\hat\rho}$ and $\boldsymbol{\hat\beta}$ we obtained the standard errors for $\boldsymbol{\hat\phi}$}.

We applied these methods to the case study data described in Section 2. Diagnostic analyses and goodness-of-fit assessment for the models fitted to the case study data set were carried out by producing bivariate residual plots with simulation polygons~\citep{moral2020bivariate}. Likelihood ratio tests were used to compare treatments and to assess the significance of the parameters.



\section{Case study analysis}


We fitted the bivariate model to the data set described in Section 2, including the effects of diet (treatment) in the linear predictors for the mean, dispersion and correlation. Table \ref{table0} shows the estimated parameters and standard errors. The females fed with \textit{D. saccharalis} emerged with greater weight ($\beta_{22}$) and had greater fertility, producing more eggs $(\beta_{12}$). The estimates for $\phi_{11}$ and $\phi_{12}$ indicate overdispersion for the number of eggs,  however since their standard errors are large we tested for their equality. The estimates for $\phi_{21}$ and $\phi_{22}$ are also similar to each other.

\begin{table}[h]
\footnotesize
\caption{Parameter estimates (EST.) and standard errors (SE) for
the normal bivariate model. $\beta_{11}$ and $\beta_{12}$ represent the mean number of eggs produced by stinkbugs on for diets \textit{Anticarsia} and \textit{Diatraea} respectively; $\beta_{21}$ and $\beta_{22}$ represent the mean weight for stinkbugs on diets \textit{Anticarsia} and \textit{Diatraea} respectively. $\phi_{11}$ and $\phi_{12}$ represent the dispersion of the number of eggs for diets \textit{Anticarsia} and \textit{Diatraea} respectively; $\phi_{21}$ and $\phi_{22}$ represent the dispersion of weight for diets \textit{Anticarsia} and \textit{Diatraea} respectively. $\rho_1$ represents the correlation between number of eggs and weight for \textit{Anticarsia} diet and the parameter $\rho_2$ represents the correlation between number of eggs and weight for \textit{Diatraea}.}
\label{table0}
\begin{center}
\begin{tabular}{lrr}
	\hline
	&EST. & SE \\ \hline
	$\beta_{11}$ & 4.0561 & 0.2907 \\ 
	$\beta_{12}$ &  5.3782 & 0.1609 \\
	
	$\beta_{21}$& 62.5000 & 4.0684  \\
	
	$\beta_{22}$& 93.9474 & 3.3824\\
	
	$\phi_{11}$& 39.0292 & 19.5049 \\ 
	$\phi_{12}$&  106.6240  & 32.1814\\
	$\phi_{21}$&  2.1186 & 1.0588  \\ 
	$\phi_{22}$& 2.3138 &0.6983\\
	$\rho_1$ & -0.0389 & 0.3528 \\
	
	$\rho_2$& 0.5192 & 0.1487\\
	\hline
	LogLik & -221.0125 & \\
	\hline
\end{tabular}
\end{center}
\end{table}


The bivariate residual plot with simulated polygons with 99 simulations showed 3 out of 27 points out of their respective polygons, which indicates a reasonably good fit (Figure \ref{fig1.2}). 




\begin{figure}[h]
\centering
\includegraphics[width=0.7\textwidth]{pod4}
\caption{Bivariate plot with simulation polygons assuming a bivariate normal
distribution for bivariate samples of size 99 simulated, with estimated marginal densities for residuals 1 and residuals 2. The
an lighter shade corresponds to the estimated density of all simulated diagnostics,
and the darker one corresponds to the estimated density of the observed
diagnostics. The points outside their simulated polygons are
displayed in red.}
\label{fig1.2}
\end{figure}



The selection of the model was performed using likelihood ratio tests. We verify the treatment effect on the mean and the dispersion by each outcome and jointly (Table \ref{table2} and \ref{table5.3}). The effect of treatment on the correlation was also tested and presented no significance, but here due to the differences in correlations observed per treatment (Figure \ref{fig1.1}), the biological assumption, and the small sample size,  we chose to maintain a different correlation effect per treatment.
The test gave no evidence for different dispersion coefficients, but different regression coefficients for each diet, hence, the regression parameters showed a difference between treatments for both outcomes, and, the diet \textit{D. saccharalis} contributed to a better development of the predator. 

%The estimates also showed high overdispersion for the number of eggs and heterogeneity of variance for the female weight, and a small correlation between outcomes.

\begin{table}[h]
\centering
\caption{Model fit measurements and comparisons between the complete and reduced models to test the treatment effect on the mean and on the dispersion for each outcome.}
\begin{tabular}{p{6cm}p{1cm}crr} \hline
Model & np & \textit{l} & 2(diff \textit{l}) & $P(> \chi^2)$ \\ 		\hline
Full model & 10 & -221.0125 && \\
\hdashline
\qquad	$\beta_{11} = \beta_{12}$   & 9 &-227.4322 & 12.8394& 0.0003\\
\qquad	$\beta_{21} = \beta_{22}$   & 9 &-236.2779 & 16.4631& 0.0000\\

\qquad $\phi_{10} = \phi_{11}$  & 9 & -222.2381 & 2.4511& 0.2936\\

\qquad  $\phi_{20} = \phi_{21}$  & 9 &-221.0235 & 0.02209& 0.9890\\


\hline
\end{tabular}

\footnote{}{np, number of parameters; \textit{l}, log-likelihood; diff \textit{l}, difference in log-likelihoods.}	

\label{table2}
\end{table}


\begin{table}[!h]
\centering
\caption{Model fit measurements and comparisons between the complete and reduced models to test the treatment effect on the mean and on the dispersion for both outcomes jointly.}
\begin{tabular}{p{6cm}p{1cm}crr} \hline
Model & np & \textit{l} & 2(diff \textit{l}) & $P(> \chi^2)$ \\ 		\hline
Full model & 10 & -221.0125 && \\
\hdashline
\qquad	$\beta_{11} = \beta_{12}$ and $\beta_{21} = \beta_{22}$   & 8 &-232.1660 & 22.3070& 0.0000\\

\qquad $\phi_{10} = \phi_{11}$ and $\phi_{20} = \phi_{21}$   & 8 & -222.2347 & 2.4445& 0.2946\\


\hline
\end{tabular}

\footnote{}{np, number of parameters; \textit{l}, log-likelihood; diff \textit{l}, difference in log-likelihoods.}	

\label{table5.3}
\end{table}


%\begin{table}[!h]
%	\centering
%	\caption{Parameter estimates (EST.) and standard errors (SE) for
%		the normal bivariate model. $\beta_{11}$ and $\beta_{12}$ represent the mean of number of eggs for diets \textit{Anticarsia} and \textit{Diatraea} respectively; $\beta_{21}$ and $\beta_{22}$ represent the mean of weight for for diets \textit{Anticarsia} and \textit{Diatraea} respectively. $\phi_{1}$  represents the dispersion mean of number of eggs for diets \textit{Anticarsia} and \textit{Diatraea}; $\phi_{2}$ represents the dispersion mean of weight for diets \textit{Anticarsia} and \textit{Diatraea}, and  $\rho$ represents the correlation between number of eggs and weight.}
%	\begin{tabular}{lrr}\Hline
%		&EST. & SE \\ 		\hline
%		$\beta_{11}$ & 4.0561 &  0.3744\\ 
%		$\beta_{12}$ &  5.3782 & 0.1430\\
%		
%		$\beta_{21}$& 62.5000 & 4.0810  \\
%		
%		$\beta_{22}$& 93.9474 & 3.3274  \\
%		$\phi_{1}$& 86.5959 & 22.5908 \\ 
%		$\phi_{2}$&  2.2560 & 0.5885  \\ 
%		$\rho$ & 0.4031  & 0.1495 \\
%		\hline
%		LogLik & -222.8309
%		& \\
%		\hline

%	\end{tabular}
%	\label{table3}
%\end{table}



\section{Simulation studies}

Simulation studies were performed to assess the properties of the maximum likelihood estimators and the flexibility of the bivariate framework. The scenarios proposed here compare the performance of the proposed model with different sample size; under, over and equidispersion for the count outcome; heterogeneity of variances for the continuous outcome and negative, positive,  low and high correlation between outcomes.

We conducted the simulation study based on the experimental structure described in the case study (Section 2). Four simulation scenarios were designed considering one treatment factor with two levels and the numbers of experimental units equal to half of the sample size. The count outcome was generated from a normal distribution with a logarithmic link function, fixing the regression coefficients at the values $\beta_{11}=2$ and $\beta_{12}=3$ (the first index represents the response variable and the second index the treatment level). The dispersion parameters were fixed at the values $\phi_{11}=0.5$ and $\phi_{12}=2$. For the second continuous outcome, we considered $\beta_{21}=10$, $\beta_{22}=300$, $\phi_{21}=1$, $\phi_{22}=10$ and an identity link function. We produced simulations based on four different values for a single correlation parameter ($\rho = -0.8, 0.2, 0.5$ and $0.8$). These scenarios present negative, weak, moderate, and strong positive correlation. To check the consistency of the estimators, four sample sizes were considered: 50, 100, 300, and 600. We generated 1000 data sets for each simulation scenario. The simulation process is summarised as follows
\begin{enumerate}
\item Define	$\boldsymbol{\beta_1}$=$\begin{bmatrix}\mathbf{2}\\
\mathbf{3}
\end{bmatrix}$; $\boldsymbol{\beta_2}$=$\begin{bmatrix}\mathbf{10}\\
\mathbf{300}
\end{bmatrix}$;	
$\boldsymbol{\phi_1}$=$\begin{bmatrix}\mathbf{0.5}\\
\mathbf{2}
\end{bmatrix}$;	
$\boldsymbol{\phi_2}$=$\begin{bmatrix}\mathbf{1}\\
\mathbf{10}
\end{bmatrix}$;\\

\item Set up $\mathbf{X}_{\beta_1}$ and  $\mathbf{X}_{\beta_2}$  as  $n \times 2$ design matrices;

\item Set up  $\mathbf{X}_{\phi_1}$ and $\mathbf{X}_{\phi_2}$ as  $n \times 2$ dispersion design matrices;


\item Calculate $\boldsymbol{\mu_1} = g^{-1}(\mathbf{X}_{\beta_1}\boldsymbol{\beta_1}) = \exp(\mathbf{X}_{\beta_1}\boldsymbol{\beta_1})$ and 
$\boldsymbol{\mu_2} = g^{-1}(\mathbf{X}_{\beta_2}\boldsymbol{\beta_2}) = \mathbf{X}_{\beta_2}\boldsymbol{\beta_2}$;

\item Obtain $\boldsymbol{\Sigma_1} = \mbox{diag}(\mathbf{X}_{\phi_1} \boldsymbol{\phi_1}) \mbox{diag} (\boldsymbol{\mu_1}) $ and	
$\boldsymbol{\Sigma_2} = \mbox{diag}(\mathbf{X}_{\phi_2} \boldsymbol{\phi_2}) \mbox{diag} (\boldsymbol{\mu_2}) $;

\item Set up  $\mathbf{X}_{\rho}$ as a $n \times 1$ unit matrix; 

\item Obtain $\boldsymbol{\Sigma}_\rho=	\mbox{diag}(\mathbf{X}_\rho{\rho})$, where $\rho$ is a scalar
(-0.8, 0.2, 0.5 or 0.8);\\

\item Calculate $	\boldsymbol{\Sigma}= \mbox{Bdiag}({\boldsymbol{\Sigma}}_1, {\boldsymbol{\Sigma}}_2)
\left[\begin{array}{cc}
\mathbf{I} & \boldsymbol{\Sigma}_\rho\\
\boldsymbol{\Sigma}_\rho & \mathbf{I} \end{array} \right]
\mbox{Bdiag}({\boldsymbol{\Sigma}}_1^{T}, \boldsymbol{{\Sigma}}_2^{T})$;\\


\item Simulate from $	\begin{bmatrix}\mathbf{Y_1}\\
\mathbf{Y_2}
\end{bmatrix} \sim  N
\begin{pmatrix}
\begin{bmatrix}
\boldsymbol{\mu_1}\\
\boldsymbol{\mu_2}
\end{bmatrix}\!\!,&
\boldsymbol{\Sigma}
\end{pmatrix}
$;\\

\item Fit the proposed model to the simulated data sets.

This process was followed for each simulation scenario.
Further, we also simulated a scenario with two correlations parameters, one for each treatment. The study considered 1000 simulations, sample sizes equal to 50, 100 and 300, and four scenarios combining correlation coefficients as follows: $\rho_1=0.2$ and $\rho_2=0.5$; $\rho_1=0.2$ and $\rho_2=0.8$; $\rho_1=-0.8$ and $\rho_2=0.2$; $\rho_1=-0.2$ and $\rho_2=0.5$.

The complete code used for the simulations and model fitting can be found in the supplementary materials.

\end{enumerate}



\subsection{Simulation results}

% In this section, we present the results of the simulation studies described above.

To visualize estimation bias for each simulation scenario Figure \ref{fig1.3}
was constructed. It presents the average bias, defined as the average estimate of 1000 simulations minus the true value and their respective confidence interval calculated as the average bias plus and minus 1.96 times the standard error.

\begin{figure}[h]
\centering
\includegraphics[width=1.1\textwidth]{Figure_2}
\caption{Estimation average bias of each simulation scenario (symbols) with their respective confidence interval of $95\%$ obtained by the average standard error considering 1000 simulations.}
\label{fig1.3}
\end{figure}

The results of the simulation studies showed that for all correlation levels, the average bias and the standard errors tend to 0 as the sample size increases, meaning that all parameters have unbiased and consistent estimators. For small samples, the dispersion parameters are underestimated. This is also observed for the regression parameters when the correlation is very high ($\rho = 0.8$). For the correlation parameters, the bias is low even for small samples, but it is smaller when the sample size increases. The average standard error is lower for strong correlations, positive or negative ($\rho = -0.8$ and $\rho = 0.8$); this is clear for the correlation coefficient, but it is also possible to observe a slightly smaller average standard error for the dispersion coefficients.
Figure \ref{fig1.4} shows the empirical coverage rate of the asymptotic confidence intervals. The results showed that for the regression parameters the empirical coverage rates are close to the nominal level of $95\%$ for all sample sizes and all simulation scenarios. For the dispersion and correlation parameters, the empirical coverage rates are lower than the nominal level, however, for $\rho = 0.2$ and $\rho = 0.5$, they become closer to the nominal level for large samples. The worst scenario is with a small sample and strong correlation, in this case, even for large samples the coverage rate is still lower than the nominal level. An alternative here is to consider other methods to construct the confidence intervals such as the bootstrap approach. 


\begin{figure}[!h]
\centering
\includegraphics[width=1\textwidth]{Figure_3}
\caption{Coverage rate based on normal approximation confidence intervals with a nominal rate of $95\%$ for different sample sizes (50, 100, 300, and 600)  and correlation coefficients (-0.8, 0.2, 0.5 and 0.8).}
\label{fig1.4}
\end{figure}

To verify the coverage rate considering bootstrap confidence intervals, a parametric bootstrap simulation was performed, considering 1000 bootstrap samples and 100 simulations, respecting the same previous scenarios with a sample size equal to 50 and correlation coefficients equal to $-0.8$ and $0.8$. These simulations showed some asymmetry and bias, mainly
for the dispersion and correlation coefficients, therefore bias-corrected percentile confidence intervals (BCP) were considered. Efron (1982) presented this method, in which the correction relies upon transforming the distribution to one that satisfies the symmetry condition~\citep{diciccio1988review,buckland1984monte}. Using this method, the
empirical coverage rates increased, becoming very close to the 95$\%$ nominal coverage (Table \ref{table5.4}).


\begin{table}[h]
\caption{Coverage rate based on  bias-corrected percentile confidence intervals with 1000 bootstrap samples and 100 simulations, $n=50$ and a nominal rate of $95\%$.}
\label{table5.4}
\begin{center}
\begin{tabular}{lrr}
	\hline
	& \multicolumn{2}{c}{Coverage rate} \\
	\hline
	Parameter	& $\rho=0.8$ & $\rho=-0.8$ \\ \hline
	$\beta_{11}$ & 0.95 & 0.96\\ 
	$\beta_{12}$ &  0.95 & 0.93 \\
	
	$\beta_{21}$& 0.98 & 0.96 \\
	
	$\beta_{22}$& 0.98 & 0.92 \\
	
	$\phi_{11}$& 0.94 & 0.95 \\ 
	$\phi_{12}$&  0.92  & 0.88\\
	$\phi_{21}$&  0.93 & 0.95 \\ 
	$\phi_{22}$& 0.91 &0.89 \\
	$\rho$ & 0.95 & 0.96\\
	\hline
\end{tabular}
\end{center}
\end{table}


Table \ref{completa} shows the results of the simulation study considering two correlations coefficients, in this example, one for each treatment.  The observed average bias was low, and the larger the sample size, the lower the average bias and associated standard errors. We observed that for stronger correlations, the standard error was lower, implying empirical confidence intervals with smaller coverage than the nominal rate. Further, it is clear that the scenarios that presented better coverage, mainly for larger samples, were scenarios 1 ($\rho=0.2$ and $\rho=0.5$) and 4 ($\rho=-0.2$ and $\rho=0.5$).



\begin{table}[!h]
\caption{Estimator mean bias, MSE and coverage rate for the proposed model under 4
scenarios, which varied true parameter value. Results are based on 1000 simulations.}\hrule\vspace{0.2cm}
\small
Scenario 1:\\
\begin{minipage}[b]{0.43\textwidth}
\label{completa}
\begin{tabular}{lrrrr} 
	& &   \multicolumn{3}{c}{Bias}  \\ 
	\cmidrule{3-5}
	
	Par.	&  True & n=50 & n=100 & n=300   \\ 			\midrule 
	$\beta_{11}$ &	2&0.000
	&-0.004&	0.000\\
	$\beta_{12}$ &	3&-0.002
	&	-0.001&	0.000\\
	$\beta_{21}$&10& -0.012
	&	-0.018&	-0.017\\
	$\beta_{22}$&300&0.154
	&	-0.059&	-0.024\\
	$\phi_{11}$	&0.5& -0.020
	&	-0.013&	-0.004\\
	$\phi_{12}$&2&	-0.093
	&	-0.054&	-0.021\\
	$\phi_{21}$	&1&	-0.044
	&	-0.015&	-0.003\\
	$\phi_{22}$	&10&-0.469
	&-0.080&	0.083	\\
	$\rho_1$	&0.2&-0.005
	&-0.004&-0.001	\\
	$\rho_2$&	0.5	&-0.013
	&-0.001&	0.000\\
\end{tabular}
\end{minipage}
\begin{minipage}[b]{0.28\textwidth}
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{MSE}  &\\ 
	\midrule
	n=50 & n=100 & n=300   \\ 	
	\midrule 
	0.050&	0.036&	0.021\\
	0.061&	0.044&	0.026\\
	0.611&	0.441&	0.257\\
	10.584&	7.669&	4.446\\
	0.133&	0.096&	0.057\\
	0.501&	0.362&	0.213\\
	0.265&	0.194&	0.114\\
	2.501&	1.844&	1.069\\
	0.179&	0.130&	0.076\\
	0.133&	0.094&	0.055\\
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.1\textwidth}
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{Coverage}  &\\ 
	\hline
	n=50 & n=100 & n=300   \\ 	
	\hline 
	0.94&	0.94&	0.94\\
	0.94&	0.93&	0.95\\
	0.93&	0.94&	0.95\\
	0.93&	0.95&	0.96\\
	0.88&	0.90&	0.92\\
	0.87&	0.90&	0.91\\
	0.87&	0.90&	0.93\\
	0.87&	0.90&	0.91\\
	0.88&	0.92&	0.93\\
	0.88&	0.89&	0.91\\
\end{tabular}
\end{minipage}
\\\hrule\vspace{0.2cm}
Scenario 2:\\
\begin{minipage}[b]{.43\textwidth}
\begin{tabular}{lrrrr} 
	& &   \multicolumn{3}{c}{Bias}  \\ 
	\cmidrule{3-5}
	Par.	&  True & n=50 & n=100 & n=300   \\ 			\midrule 
	$\beta_{11}$ &	2&-0.002&	-0.001&	0.001\\
	$\beta_{12}$ &	3&-0.002&	0.001&	0.001\\
	$\beta_{21}$&10& 0.021&	0.006&	-0.004\\
	$\beta_{22}$&300&-0.093&	0.207&	0.078\\
	$\phi_{11}$	&0.5&-0.015&	-0.014&	-0.005\\
	$\phi_{12}$&2&	-0.059&	-0.048&	-0.031\\
	$\phi_{21}$	&1&	-0.036&	-0.021&	-0.012\\
	$\phi_{22}$	&10&-0.283&	-0.281&	-0.130\\
	$\rho_1$	&0.2&-0.011&	0.003&	0.000\\
	$\rho_2$&	0.8	&-0.006&	-0.002&	-0.002\\
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.28\textwidth}
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{MSE} & \\ 
	\hline
	n=50 & n=100 & n=300   \\ 	
	\midrule 
	0.051&	0.036&	0.021\\
	0.062&	0.044&	0.026\\
	0.614&	0.440&	0.256\\
	10.682&	7.599&	4.435\\
	0.134&	0.096&	0.057\\
	0.450&	0.321&	0.187\\
	0.267&	0.193&	0.113\\
	2.252&	1.597&	0.939\\
	0.180&	0.129&	0.076\\
	0.058&	0.040&	0.023\\
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.1\textwidth}
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{Coverage} & \\ 
	\hline
	n=50 & n=100 & n=300   \\ 	
	\midrule 
	0.91&	0.95&	0.94\\
	0.92&	0.94&	0.94\\
	0.94&	0.94&	0.94\\
	0.92&	0.94&	0.95\\
	0.89&	0.89&	0.93\\
	0.86&	0.87&	0.87\\
	0.89&	0.91&	0.93\\
	0.86&	0.86&	0.88\\
	0.90&	0.91&	0.95\\
	0.83&	0.86&	0.87\\ 		
\end{tabular}
\end{minipage}
\\\hrule\vspace{0.2cm}
Scenario 3:\\
\begin{minipage}[b]{.43\textwidth}	
\begin{tabular}{lrrrr} 
	& &   \multicolumn{3}{c}{Bias}  \\ 
	\cmidrule{3-5}
	Par.	&  True & n=50 & n=100 & n=300   \\ 			\midrule 
	$\beta_{11}$ &	2&-0.001&	0.000&	0.000\\
	$\beta_{12}$ &	3&0.001&	0.001&	0.001\\
	$\beta_{21}$&10&-0.013&	-0.025&	-0.007\\
	$\beta_{22}$&300&-0.149&	-0.321&	0.245\\
	$\phi_{11}$	&0.5& -0.014&	-0.014&	-0.001\\
	$\phi_{12}$&2&	-0.109&	-0.050&	-0.029\\
	$\phi_{21}$	&1&	-0.030&	-0.010&	-0.002\\
	$\phi_{22}$	&10&-0.239&	-0.268&	-0.100\\
	$\rho_1$ &-0.8	&0.005&	0.007&	0.002\\
	$\rho_2$&	0.2	&
	0.004&	-0.003&	-0.004\\
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.28\textwidth} 		
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{MSE}  &\\ 
	\hline
	n=50 & n=100 & n=300   \\ 	
	\hline 
	0.051&	0.036&	0.021\\
	0.061&	0.044&	0.026\\
	0.615&	0.442&	0.257\\
	10.699&	7.599&	4.444\\
	0.112&	0.080&	0.047\\
	0.524&	0.384&	0.225\\
	0.225&	0.163&	0.095\\
	2.701&	1.918&	1.130\\
	0.057&	0.041&	0.023\\
	0.178&	0.130&	0.076\\ 	
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.1\textwidth} 		
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{Coverage} &  \\ 
\hline
	n=50 & n=100 & n=300   \\ 	
	\midrule 
	0.94&	0.93&	0.93\\
	0.92&	0.94&	0.95\\
	0.92&	0.93&	0.95\\
	0.94&	0.93&	0.94\\
	0.83&	0.85&	0.89\\
	0.86&	0.91&	0.93\\
	0.84&	0.86&	0.89\\
	0.88&	0.91&	0.94\\
	0.85&	0.84&	0.85\\
	0.88&	0.93&	0.94\\	
\end{tabular}
\end{minipage}
\\\hrule\vspace{0.2cm}
Scenario 4:\\
\begin{minipage}[b]{.43\textwidth}
\begin{tabular}{lrrrr} 
	& &   \multicolumn{3}{c}{Bias}  \\ 
	\cmidrule{3-5}
	Par.	&  True & n=50 & n=100 & n=300   \\ 			\midrule 
	$\beta_{11}$ &	2&-0.003&	0.001&	-0.001\\
	$\beta_{12}$ &	3&-0.001&	0.002&	0.001\\
	$\beta_{21}$&10& -0.041&	-0.013&	-0.002\\
	$\beta_{22}$&300&-0.137&	-0.179&	0.361\\
	$\phi_{11}$	&0.5&-0.017&	-0.010&	-0.004\\
	$\phi_{12}$&2&-0.085&	-0.062&	-0.035\\
	$\phi_{21}$	&1&-0.035&	-0.014&	-0.009\\
	$\phi_{22}$	&10&-0.448&	-0.264&	-0.071\\
	$\rho_1$	&-0.2&0.004&	0.000&	0.002\\
	$\rho_2$&	0.5	&-0.015&	-0.008&	-0.005\\
	\hline
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.28\textwidth}
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{MSE}  &\\
	\hline
	n=50 & n=100 & n=300   \\ 	
	\midrule 
	0.051&	0.036&	0.021\\
	0.061&	0.044&	0.025\\
	0.612&	0.441&	0.256\\
	10.583&	7.597&	4.450\\
	0.134&	0.097&	0.057\\
	0.503&	0.361&	0.212\\
	0.267&	0.194&	0.113\\
	2.506&	1.815&	1.072\\
	0.179&	0.130&	0.076\\
	0.134&	0.095&	0.055\\	
	\hline
\end{tabular}
\end{minipage}
\begin{minipage}[b]{.1\textwidth}	
\begin{tabular}{rrr} 
	& \multicolumn{1}{c}{Coverage} &  \\ 
	\hline
	n=50 & n=100 & n=300   \\ 	
	\hline 
	0.94&	0.94&	0.94\\
	0.94&	0.94&	0.95\\
	0.93&	0.96&	0.94\\
	0.94&	0.95&	0.95\\
	0.88&	0.91&	0.92\\
	0.88&	0.90&	0.91\\
	0.88&	0.92&	0.91\\
	0.86&	0.88&	0.91\\
	0.88&	0.94&	0.93\\
	0.88&	0.90&	0.90\\	
	\hline
\end{tabular}
\end{minipage}
\end{table}












\section{Discussion}
\label{s:discuss}

In this work, we proposed a bivariate framework to jointly model count and continuous responses.
Our approach represents an attractive and flexible framework for studying two variables jointly and is widely applicable.
This alternative considered a normal approach, modelling the variance-covariance matrix so that the characteristics of the data, such as over- or underdispersion for count data, and heterogeneity of variances for continuous data can be modeled. However, considering the multivariate normal approximation the estimation performance is better with larger samples. The advantage of this methodology is that it is possible to incorporate correlation, jointly modeling the variable responses and assuming an associated probability distribution. One drawback of our approach is that issues may arise for small sample sizes, including biased dispersion estimates and a true coverage rate of the confidence intervals for the parameters smaller than the nominal rate. So it is necessary to carefully analyze very small samples, doing descriptive analyzes before fitting the model, and check if the obtained results have a practical interpretation.

We observed in the simulation studies that even for large samples, when $\rho = -0.8$ and $\rho = 0.8$,  the coverage rate of the correlation parameters is close to $90\%$ instead of $95\%$. This is possibly because the true correlation value is close to the boundary of the parametric space.
When  $\rho \approx 1$,  the range limits are automatically limited by 1 or - 1, then the variation of the parameter space is smaller. This became more evident by observing Figure \ref {fig1.3}, which showed that the standard errors for the correlation coefficients are lower for higher correlations.
This is not reflected in the regression coefficients, due to the fact that $\boldsymbol{\hat\beta}$ varies very little for different values of $\phi$ and $\rho$. However, the coverage rate for the dispersion parameters also depends on the correlation parameter level, and the higher the correlation, the lower the coverage rate.  These cited problems are expected when it comes to estimates at the limit of the parametric space, especially with small samples. Despite these limitations, we concluded that the model generally behaves as expected, with unbiased, consistent, and accurate estimates.


Several extensions to the proposed bivariate model are possible (and currently in progress). These extensions include modeling correlation between observations taken on the same experimental unit, e.g., longitudinal data or repeated measurements. In this case, the variance-covariance matrix needs to be extended, allowing to incorporate the dependent measures taken on the same subject.  This extension will be carried out by adding in the current $\boldsymbol{\Sigma} = \mathbf{D_\phi D_\mu}$ the term $\mathbf{ZGZ'}$, where $\mathbf{Z}$ is the matrix of known covariates indicating the correlated observations or how the response evolves overtime for the $i$th subject and $\mathbf{G}$ a general covariance matrix. However, it is also necessary to include a link function for the dispersion parameter, so that it does not have a negative maximum likelihood estimate. 
%  Here, we create the bibliographic entries manually, following the
%  journal style.  If you use this method or use natbib, PLEASE PAY
%  CAREFUL ATTENTION TO THE BIBLIOGRAPHIC STYLE IN A RECENT ISSUE OF
%  THE JOURNAL AND FOLLOW IT!  Failure to follow stylistic conventions
%  just lengthens the time spend copyediting your paper and hence its
%  position in the publication queue should it be accepted.

%  We greatly prefer that you incorporate the references for your
%  article into the body of the article as we have done here 
%  (you can use natbib or not as you choose) than use BiBTeX,
%  so that your article is self-contained in one file.
%  If you do use BiBTeX, please use the .bst file that comes with 
%  the distribution.  In this case, replace the thebibliography
%  environment below by 
%
\bibliographystyle{biom} \bibliography{Cap2}



%  If your paper refers to supporting web material, then you MUST
%  include this section!!  See Instructions for Authors at the journal
%  website http://www.biometrics.tibs.org



%\section*{Supporting Information}

%Web ix A, referenced in Section~\ref{s:model}, is available with
%this paper at the Biometrics website on Wiley Online
%Library.\vspace*{-8pt}

\appendix


	 \renewcommand{\theequation}{A.\arabic{equation}}
	% redefine the command that creates the equation no.
	\setcounter{equation}{0}  % reset counter 
	
\section*{Appendix A: Estimation an Inference for Section 4}

\textbf{Univariate modelling}

Taking one of the response outcomes $\mathbf {Y} _i $, from here we will omit the index $ i $ to improve readability. Asymptotically  $\boldsymbol{Y} \sim N({\boldsymbol{\mu}},\boldsymbol{\Sigma}= \mathbf{D_\phi}\mathbf{D_\mu})$, we can write the log-likelihood as
\begin{equation}\label{eq12}
\begin{array}{l}
\log \mathcal{L}(\boldsymbol{\mu},\boldsymbol{\Sigma}| \mathbf{y})= l(\boldsymbol{\mu},\boldsymbol{\Sigma}| \mathbf{y})= -\frac{1}{2}\{\ln |\boldsymbol{\Sigma}| +  (\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1} (\mathbf{y}-\boldsymbol{\mu})\} + \mbox{const.}



%= -\frac{1}{2}\log |\mathbf{D_\phi}\mathbf{D_\mu}| - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu})\\~\\

%  =	\{- \frac{1}{2}\log|\mathbf{D_\phi}|-  \frac{1}{2} \log|\mathbf{D_\mu}|  - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu}) \}

\end{array}
\end{equation}

Differentiating with respect to $\boldsymbol{\Sigma}$ is straghtforward:
\begin{equation}
\begin{array}{l}
\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} =\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} \{-\frac{1}{2}\ln |\boldsymbol{\Sigma}| - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-1} (\mathbf{y}-\boldsymbol{\mu})\}



%= -\frac{1}{2}\log |\mathbf{D_\phi}\mathbf{D_\mu}| - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu})\\~\\

%  =	\{- \frac{1}{2}\log|\mathbf{D_\phi}|-  \frac{1}{2} \log|\mathbf{D_\mu}|  - \frac{1}{2} (\mathbf{y}-\boldsymbol{\mu})'(\mathbf{D_\phi}\mathbf{D_\mu})^{-1} (\mathbf{y}-\boldsymbol{\mu}) \}
\end{array}
\end{equation}\\
as $\dfrac{\partial}{\partial A} \ln|A| = A^{-T} \quad \mbox{and} \quad \dfrac{\partial \mathbf{a^{T} X^{-1} b}}{\partial \mathbf{X}}  = -\mathbf{X}^{-T} \mathbf{ab^{T} X ^{-T}}$. Then, 

\begin{equation}
\begin{array}{l}
\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} = -\frac{1}{2}\boldsymbol{\Sigma}^{-T}+\frac{1}{2}\boldsymbol{\Sigma}^{-T}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-T}

\end{array}
\end{equation}
and
\begin{equation*}
\begin{array}{l}
\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} \dfrac{\partial \boldsymbol{\Sigma}}{\partial \mathbf{D_\phi}}= \left[-\frac{1}{2}\boldsymbol{\Sigma}^{-T}+\frac{1}{2}\boldsymbol{\Sigma}^{-T}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\boldsymbol{\Sigma}^{-T}\right] \circ \mathbf{D_\mu}\\~\\

= \left[-\frac{1}{2}\mathbf{(D_\phi D_\mu)}^{-T}+\frac{1}{2}\mathbf{(D_\phi D_\mu)}^{-T}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mathbf{(D_\phi D_\mu)}^{-T}\right] \circ \mathbf{D_\mu}\\~\\

= -\frac{1}{2}\mathbf{(D_\phi D_\mu)}^{-T}\circ\mathbf{D_\mu}+\frac{1}{2}\mathbf{(D_\phi D_\mu)}^{-T}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mathbf{(D_\phi D_\mu)}^{-T}\circ\mathbf{D_\mu} \\~\\

= -\frac{1}{2}\mathbf{ D_\phi^{-T}D_\mu^{-T}}\circ\mathbf{D_\mu}+\frac{1}{2}\mathbf{ D_\phi^{-T}D_\mu^{-T}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mathbf{ D_\phi^{-T}D_\mu^{-T}}\circ\mathbf{D_\mu} \\~\\

= -\frac{1}{2}\mathbf{ D_\phi^{-T}}+\frac{1}{2}\mathbf{ D_\phi^{-T}D_\mu^{-T}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mathbf{ D_\phi^{-T}}.

\end{array}
\end{equation*}
If \textbf{A} is diagonal, then $\textbf{A}^T = \textbf{A}$ and

\begin{equation*}
\begin{array}{l}
\dfrac{\partial l}{\partial \boldsymbol{\Sigma}} \dfrac{\partial \boldsymbol{\Sigma}}{\partial \mathbf{D_\phi}}\dfrac{\partial \mathbf{D_\phi}}{\partial \boldsymbol{\phi}}=[-\frac{1}{2}\mathbf{ D_\phi^{-1}}+\frac{1}{2}\mathbf{ D_\phi^{-1}D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mathbf{ D_\phi^{-1}}]\circ \mathbb{I}\\~\\     
\end{array}
\end{equation*}
\begin{equation}\label{eq3}
= -\frac{1}{2}\mbox{diag}\mathbf{ (X_\phi \boldsymbol{\phi})^{-1}}\circ \mathbb{I}+\frac{1}{2}\mbox{diag}\mathbf{ (X_\phi \boldsymbol{\phi})^{-1}D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mbox{diag}\mathbf{( X_\phi \boldsymbol{\phi})^{-1}}\circ \mathbb{I}.
\end{equation}\\
Equalizing (\ref{eq3}) to zero

\begin{equation}\label{eqphi}
\begin{array}{l}
-\frac{1}{2}\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})^{-1}}\circ \mathbb{I}+\frac{1}{2}\mbox{diag}\boldsymbol{ (X_\phi \hat{\phi})^{-1}\mathbf{D}_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mbox{diag}\boldsymbol{( \mathbf{X}_\phi \hat{\phi})^{-1}}\circ \mathbb{I} =0 \nonumber\\~\\

\therefore\mbox{diag}\boldsymbol{ (X_\phi \hat{\phi})^{-1}}\circ \mathbb{I}=\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})^{-1}\mathbf{D}_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\mbox{diag}\boldsymbol{( \mathbf{X}_\phi \hat{\phi})^{-1}} \nonumber\circ \mathbb{I}\\~\\

%\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})}\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})^{-1}}\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})}=\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})}\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})^{-1}\mathbf{D}_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\circ \mathbb{I}\\\hspace{13cm}\mbox{diag}\boldsymbol{( \mathbf{X}_\phi \hat{\phi})^{-1}}\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})}\nonumber \\~\\


\therefore\mbox{diag}\boldsymbol{ (\mathbf{X}_\phi \hat{\phi})}=\mathbf{D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\circ \mathbb{I}\nonumber.\\

\end{array}
\end{equation}\\
Multiplying by $\mathbb{J}_{n \times 1}$ in both sides and factoring out $\boldsymbol{\hat\phi}$ yields
\begin{equation}
\boldsymbol{\hat{\phi}} = \mathbf{(X_\phi'X_\phi)^{-1}X_\phi'}\mathbf{\{(D_\mu^{-1}}(\mathbf{y}-\boldsymbol{\mu})(\mathbf{y}-\boldsymbol{\mu})'\circ \mathbb{I}) \mathbb{J}\} 
\end{equation}
where $\mathbb{I}$ is an $n \times n$ identity matrix and $\mathbb{J}$ is an $n \times n$ vector of 1s, and $\circ$ the Hadamard product.


%\captionsetup{labelformat=AppendixTables}
%\setcounter{table}{0}

%We presented all simulation studies carried out with the proposed model. We investigated
%model performance for two values of $\rho$ under 14 different scenarios, varying sample
%size, from 1000 simulations. We obtained the bias, the  mean square error (MSE) and the coverage rate(see Table 5).









\end{document}
